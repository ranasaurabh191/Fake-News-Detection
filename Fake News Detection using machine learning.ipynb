{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a0802a",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34865a8e",
   "metadata": {},
   "source": [
    "## Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b507f",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb35954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake=pd.read_csv('Fake.csv')\n",
    "data_true=pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c382f",
   "metadata": {},
   "source": [
    "### Data Preview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6203eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3041d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake[\"class\"]=0\n",
    "data_true['class']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe3918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_fake.shape, data_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2332b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake_manual_testing = data_fake.tail(10)\n",
    "for i in range(23480,23470,-1):\n",
    "    data_fake.drop([i],axis = 0, inplace = True)\n",
    "\n",
    "    \n",
    "data_true_manual_testing = data_true.tail(10)\n",
    "for i in range(21416,21406,-1):\n",
    "    data_true.drop([i],axis = 0, inplace = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f6d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_fake.shape, data_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake_manual_testing['class']=0\n",
    "data_true_manual_testing['class']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15140120",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake_manual_testing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7531db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true_manual_testing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge=pd.concat([data_fake, data_true], axis = 0)\n",
    "data_merge.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1cd410",
   "metadata": {},
   "source": [
    "#### \"title\",  \"subject\" and \"date\" columns is not required for detecting the fake news, so I am going to drop the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adccb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_merge.drop(['title','subject','date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of missing values\n",
    "data.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe0118",
   "metadata": {},
   "source": [
    "#### Randomly shuffling the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe609699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e879eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70930a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20157149",
   "metadata": {},
   "source": [
    "## Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c82e01",
   "metadata": {},
   "source": [
    "#### Creating a function to convert the text in lowercase, remove the extra space, special chr., url and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453dfdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "    text = re.sub('<.*?>+',b'',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
    "    text = re.sub('\\w*\\d\\w*','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(wordopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a11e07",
   "metadata": {},
   "source": [
    "#### Defining dependent and independent variable as x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['text']\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9aac19",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c3c72",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training set and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645dd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3169fde",
   "metadata": {},
   "source": [
    "### Extracting Features from the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddd784",
   "metadata": {},
   "source": [
    "#### Convert text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cfbfc1",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42070f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = LR.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f671957",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc114e19",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt = DT.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442fa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT.score(xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e62d57",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GB = GradientBoostingClassifier(random_state = 0)\n",
    "GB.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c623e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gb = GB.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB.score(xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d83e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da9186",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954338d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(random_state = 0)\n",
    "RF.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb15f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = RF.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.score(xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test, pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d58d0",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lable(n):\n",
    "    if n==0:\n",
    "        return \"Fake News\"\n",
    "    elif n==1:\n",
    "        return \"Not A Fake News\"\n",
    "    \n",
    "def manual_testing(news):\n",
    "    testing_news = {\"text\":[news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    new_def_test['text'] = new_def_test[\"text\"].apply(wordopt)\n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    new_xv_test = vectorization.transform(new_x_test)\n",
    "    pred_LR = LR.predict(new_xv_test)\n",
    "    pred_DT = DT.predict(new_xv_test)\n",
    "    pred_GB = GB.predict(new_xv_test)\n",
    "    pred_RF = RF.predict(new_xv_test)\n",
    "    \n",
    "    return print(\"\\n\\nLR Predicition: {} \\nDT Prediction: {} \\nGBC Prediction: {} \\nRFC Prediction:{}\".format(output_lable(pred_LR[0]),\n",
    "                                                                                                             output_lable(pred_DT[0]),\n",
    "                                                                                                             output_lable(pred_GB[0]),\n",
    "                                                                                                             output_lable(pred_RF[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308fb1b",
   "metadata": {},
   "source": [
    "### Model Testing With Manual Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "# Function to clean the text\n",
    "\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'$$.*?$$', '', text)  # Remove text within square brackets\n",
    "    text = re.sub(r\"\\W\", \" \", text)      # Remove non-word characters\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>+', '', text)   # Remove HTML tags\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words containing numbers\n",
    "    return text\n",
    "# Load your datasets\n",
    "data_fake = pd.read_csv('Fake.csv')\n",
    "data_true = pd.read_csv('True.csv')\n",
    "\n",
    "# Assign class labels\n",
    "data_fake[\"class\"] = 0\n",
    "data_true[\"class\"] = 1\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([data_fake, data_true], axis=0)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Clean the text data\n",
    "data['text'] = data['text'].apply(wordopt)\n",
    "\n",
    "# Split the data\n",
    "x = data['text']\n",
    "y = data['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, stratify=y)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)\n",
    "\n",
    "# Train models\n",
    "LR = LogisticRegression()\n",
    "LR.fit(xv_train, y_train)\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(xv_train, y_train)\n",
    "\n",
    "GB = GradientBoostingClassifier(random_state=0)\n",
    "GB.fit(xv_train, y_train)\n",
    "\n",
    "RF = RandomForestClassifier(random_state=0)\n",
    "RF.fit(xv_train, y_train)\n",
    "\n",
    "# Save the vectorizer and models\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorization, f)\n",
    "\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(LR, f)\n",
    "\n",
    "with open('decision_tree_model.pkl', 'wb') as f:\n",
    "    pickle.dump(DT, f)\n",
    "\n",
    "with open('gradient_boosting_model.pkl', 'wb') as f:\n",
    "    pickle.dump(GB, f)\n",
    "\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(RF, f)\n",
    "\n",
    "print(\"Models and vectorizer saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
